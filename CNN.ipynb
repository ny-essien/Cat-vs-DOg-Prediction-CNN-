{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea35d348-98cd-48a1-88d1-a0a0603cd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd464ba2-aae8-4e37-96b0-20f35504f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"c://ml_dataset/CNN/dataset/training_set\"\n",
    "path_test = \"c://ml_dataset/CNN/dataset/test_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9aa5a47-3bbd-4b11-aa23-ec3ef9b3d14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the Training Set\n",
    "#rescale = feature scaling on the training set\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True        \n",
    ")\n",
    "\n",
    "#connecting the train_datagen to the training set\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "                path_train,\n",
    "                target_size =  (64,64),\n",
    "                batch_size = 32,\n",
    "                class_mode = 'binary'\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1caeaaed-da87-4a71-ac3d-7dd022066908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#preprocessing the test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#connecting the test_datagen to the test set\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "\n",
    "            path_test,\n",
    "            target_size =  (64,64),\n",
    "            batch_size = 32,\n",
    "            class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609eaef1-4933-40ab-ae60-683c09df9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN\n",
    "cnn = tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7574fe-c24a-40a9-82dd-20c332ef374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution\n",
    "#Adding a convolutional layer\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\", input_shape = [64,64,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22ff3cc-56c2-4ca1-a4b0-2adfd1e79ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pooling\n",
    "#Adding the Pooling Layer\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2,3), strides = 2, padding = 'valid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cde7324-81df-4934-9301-3d2c0561df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding another convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = \"relu\" ))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2,3), strides = 2, padding = 'valid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a441a65-ddf9-4786-9f4f-84f54b8e3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b1cd8f-ae92-48a0-8cfa-14b28df53a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Connection\n",
    "#Adding the fully connected layers\n",
    "cnn.add(tf.keras.layers.Dense( units = 128, activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220abaa3-78a5-4599-a471-7295ab251944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29fc61f5-a953-42a9-9d61-37ae68990c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81bbdb10-c58a-4747-94c3-e6fbe96b7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the CNN\n",
    "cnn.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ef930e-c148-4884-b4d1-a26d843291be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 400s 2s/step - loss: 0.6687 - accuracy: 0.5888 - val_loss: 0.6204 - val_accuracy: 0.6555\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 0.5961 - accuracy: 0.6809 - val_loss: 0.6215 - val_accuracy: 0.6565\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.5602 - accuracy: 0.7049 - val_loss: 0.5271 - val_accuracy: 0.7445\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.5286 - accuracy: 0.7326 - val_loss: 0.5485 - val_accuracy: 0.7310\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.5083 - accuracy: 0.7485 - val_loss: 0.5021 - val_accuracy: 0.7710\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.4899 - accuracy: 0.7645 - val_loss: 0.4899 - val_accuracy: 0.7845\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.4667 - accuracy: 0.7730 - val_loss: 0.4778 - val_accuracy: 0.7770\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.4529 - accuracy: 0.7829 - val_loss: 0.4783 - val_accuracy: 0.7885\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.4418 - accuracy: 0.7816 - val_loss: 0.4767 - val_accuracy: 0.7880\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.4331 - accuracy: 0.7930 - val_loss: 0.5489 - val_accuracy: 0.7445\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.4188 - accuracy: 0.8075 - val_loss: 0.4891 - val_accuracy: 0.7800\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.4054 - accuracy: 0.8121 - val_loss: 0.4946 - val_accuracy: 0.7810\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 45s 178ms/step - loss: 0.3938 - accuracy: 0.8255 - val_loss: 0.5861 - val_accuracy: 0.7545\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3881 - accuracy: 0.8188 - val_loss: 0.4445 - val_accuracy: 0.8115\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 62s 250ms/step - loss: 0.3781 - accuracy: 0.8250 - val_loss: 0.4547 - val_accuracy: 0.8000\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 61s 244ms/step - loss: 0.3654 - accuracy: 0.8321 - val_loss: 0.4476 - val_accuracy: 0.8150\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.3542 - accuracy: 0.8367 - val_loss: 0.4421 - val_accuracy: 0.8025\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3346 - accuracy: 0.8501 - val_loss: 0.4834 - val_accuracy: 0.8105\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.3221 - accuracy: 0.8608 - val_loss: 0.4881 - val_accuracy: 0.8020\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.3208 - accuracy: 0.8547 - val_loss: 0.4617 - val_accuracy: 0.7995\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.3095 - accuracy: 0.8677 - val_loss: 0.4540 - val_accuracy: 0.8130\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.3042 - accuracy: 0.8658 - val_loss: 0.4527 - val_accuracy: 0.8150\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.2841 - accuracy: 0.8754 - val_loss: 0.4809 - val_accuracy: 0.8095\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.2810 - accuracy: 0.8816 - val_loss: 0.4810 - val_accuracy: 0.8090\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.2753 - accuracy: 0.8865 - val_loss: 0.4616 - val_accuracy: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b3cb5748b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the CNN on the training set and evaluating it in the test set\n",
    "#Training on the training set and validating on the test set\n",
    "\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs =  25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a1ef78-3ae5-4740-98b7-1effb4d367af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29e0001-23ce-4780-90a3-5e517dcf6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 304ms/step\n",
      "Cat\n"
     ]
    }
   ],
   "source": [
    "path = \"c://ml_dataset/CNN/dataset/single_prediction/cat_or_dog2.jpg\"\n",
    "test_image = tf.keras.utils.load_img(path, target_size = (64,64))\n",
    "\n",
    "#Converting the image in PIL format to a 2D(numpy) array\n",
    "test_image = tf.keras.utils.img_to_array(test_image)\n",
    "#adding an extra dimension corresponding to the batch\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "#Getting prediction result\n",
    "result = cnn.predict(test_image)\n",
    "#Getting what indices correspond to each category\n",
    "training_set.class_indices\n",
    "\n",
    "#getting a nice output at the end\n",
    "#Accessing the batch and the single element of the batch\n",
    "if result[0][0] == 1:\n",
    "    prediction = \"Dog\"\n",
    "    \n",
    "else:\n",
    "    prediction = \"Cat\"\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0f935-a4d6-4a94-8d95-91cc83797b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
